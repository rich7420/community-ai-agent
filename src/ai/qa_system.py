"""
Q&A System combining RAG and LLM
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timezone
import json

from langchain.chains import LLMChain
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.base import BaseCallbackHandler

from ai.google_llm import GoogleLLM
from ai.rag_system import CommunityRAGSystem
from ai.prompts import CommunityPrompts
from utils.user_display_helper import UserDisplayHelper
from utils.logging_config import structured_logger
from mcp.user_stats_mcp import get_slack_user_stats, get_slack_activity_summary
from mcp.calendar_mcp_fixed import CalendarMCPFixed
from cache.answer_cache import get_cache

logger = logging.getLogger(__name__)


class CommunityQASystem:
    """Q&A system for community data using RAG + LLM"""
    
    def __init__(
        self,
        rag_system: CommunityRAGSystem,
        llm: GoogleLLM,
        memory_size: int = 10,
        max_context_length: int = 4000
    ):
        """
        Initialize Q&A system
        
        Args:
            rag_system: RAG system for document retrieval
            llm: Language model for generation
            memory_size: Number of conversation turns to remember
            max_context_length: Maximum context length for LLM
        """
        self.rag_system = rag_system
        self.llm = llm
        self.max_context_length = max_context_length
        
        # Initialize conversation memory
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Initialize cache
        self.cache = get_cache()
        
        # Initialize MCP services
        self.calendar_mcp = CalendarMCPFixed()
        self.user_display_helper = UserDisplayHelper()
        
        # Initialize prompt templates
        self.qa_prompt = CommunityPrompts.get_qa_prompt()
        
        # Initialize LLM chain
        self.qa_chain = LLMChain(
            llm=llm,
            prompt=self.qa_prompt,
            memory=self.memory,
            verbose=True
        )
        
        structured_logger.info("Q&A system initialized", extra={
            "memory_size": memory_size,
            "max_context_length": max_context_length
        })

    def ask_question(
        self,
        question: str,
        k: int = 5,  # å¢žåŠ åˆ°5å€‹æ–‡æª”
        score_threshold: float = 0.5,
        source_filter: Optional[str] = None,
        include_sources: bool = True
    ) -> Dict[str, Any]:
        """
        Ask a question and get an answer using RAG
        
        Args:
            question: The question to ask
            k: Number of documents to retrieve
            score_threshold: Minimum similarity score for documents
            source_filter: Optional source type filter (slack, github, etc.)
            include_sources: Whether to include source information
            
        Returns:
            Dictionary with answer, sources, and metadata
        """
        try:
            # Check cache first
            cached_result = self.cache.get_cached_answer(question)
            if cached_result:
                structured_logger.info("Using cached answer", extra={
                    "question": question[:50] + "..." if len(question) > 50 else question,
                    "cached": True
                })
                return {
                    "question": question,
                    "answer": cached_result["answer"],
                    "sources": cached_result["sources"],
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": len(cached_result["sources"]),
                    "context_length": len(cached_result["answer"]),
                    "cached": True,
                    "cache_timestamp": cached_result["cache_timestamp"]
                }
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºçµ±è¨ˆé¡žå•é¡Œ
            if self._is_stats_question(question):
                return self._handle_stats_question(question)
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç”¨æˆ¶æ´»èºåº¦æŸ¥è©¢
            if self._is_user_activity_query(question):
                return self._handle_user_activity_query(question)
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ—¥æ›†æŸ¥è©¢
            if self._is_calendar_query(question):
                return self._handle_calendar_query(question)
            
            # Prepare filter for RAG
            filter_dict = None
            if source_filter:
                filter_dict = {"source_type": source_filter}
            
            # Retrieve enhanced context including project descriptions
            context = self.rag_system.get_enhanced_context(
                query=question,
                k=k
            )
            
            # Also get relevant documents for sources if needed
            relevant_docs = self.rag_system.get_relevant_documents(
                query=question,
                k=k,
                score_threshold=score_threshold,
                filter=filter_dict
            )
            
            # Prepare input for LLM
            chain_input = {
                "question": question,
                "context": context
            }
            
            # Generate answer
            try:
                # Direct LLM call (simplified approach)
                prompt = f"{CommunityPrompts.QA_SYSTEM_PROMPT}\n\n{CommunityPrompts.QA_HUMAN_PROMPT}".format(
                    context=context,
                    question=question
                )
                answer = self.llm._call(prompt)
                if hasattr(answer, 'content'):
                    answer = answer.content
                else:
                    answer = str(answer)
                
                # ååŒ¿ååŒ–ç”¨æˆ¶åç¨±ä¸¦è§£æžç”¨æˆ¶å¼•ç”¨
                from utils.pii_filter import PIIFilter
                pii_filter = PIIFilter()
                answer = pii_filter.resolve_user_references(answer)
                
            except Exception as direct_error:
                logger.error(f"Direct LLM call failed: {direct_error}")
                answer = "I apologize, but I'm experiencing technical difficulties. Please try again later."
            
            # Prepare response
            result = {
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat(),
                "sources_used": len(relevant_docs),
                "context_length": len(context)
            }
            
            if include_sources:
                sources = [
                    {
                        "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                        "metadata": doc["metadata"],
                        "score": doc["score"]
                    }
                    for doc in relevant_docs
                ]
                result["sources"] = sources
                
                # Cache the answer for future use
                self.cache.cache_answer(question, answer, sources)
            
            structured_logger.info("Question answered", extra={
                "question": question,
                "answer_length": len(answer),
                "sources_used": len(relevant_docs),
                "source_filter": source_filter
            })
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to answer question: {e}")
            return {
                "question": question,
                "answer": f"Sorry, I encountered an error while processing your question: {str(e)}",
                "error": True,
                "timestamp": datetime.now().isoformat()
            }
    
    def _is_stats_question(self, question: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºçµ±è¨ˆé¡žå•é¡Œ"""
        stats_keywords = [
            "æœ€æ´»èº", "æ´»èº", "å‰ä¸‰å€‹", "å‰å¹¾å", "æœ€å¤š", "çµ±è¨ˆ", 
            "ç™¼è¨Šæ¯", "å›žè¦†", "emoji", "ä½¿ç”¨æ¬¡æ•¸", "æ´»å‹•é‡"
        ]
        question_lower = question.lower()
        return any(keyword in question_lower for keyword in stats_keywords)
    
    def _is_user_activity_query(self, question: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºç”¨æˆ¶æ´»èºåº¦æŸ¥è©¢"""
        # æ›´ç²¾ç¢ºçš„ç”¨æˆ¶æŸ¥è©¢æ¨¡å¼
        user_query_patterns = [
            r'(.+?)æ˜¯èª°[ï¼Ÿ?]?$',  # "XXXæ˜¯èª°ï¼Ÿ"
            r'(.+?)çš„æ´»èºåº¦',     # "XXXçš„æ´»èºåº¦"
            r'(.+?)çš„è¨Žè«–',       # "XXXçš„è¨Žè«–"
            r'(.+?)çš„åƒèˆ‡',       # "XXXçš„åƒèˆ‡"
            r'(.+?)ç™¼äº†å¤šå°‘',     # "XXXç™¼äº†å¤šå°‘"
            r'(.+?)çš„è¨Šæ¯',       # "XXXçš„è¨Šæ¯"
            r'(.+?)çš„è²¢ç»',       # "XXXçš„è²¢ç»"
        ]
        
        import re
        for pattern in user_query_patterns:
            if re.search(pattern, question):
                return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å…·é«”çš„ç”¨æˆ¶åç¨±ï¼ˆä¸­æ–‡å§“åã€è‹±æ–‡å§“åã€å·²çŸ¥çš„mentoråç¨±ï¼‰
        known_users = [
            "è”¡å˜‰å¹³", "å˜‰å¹³", "Jesse", "èŽŠå‰èµ³", "å‰èµ³", "åŠ‰å“²ä½‘", "Jason", 
            "å¤§ç¥ž", "å¤§ä½¬", "mentor", "mentors"
        ]
        
        question_lower = question.lower()
        for user in known_users:
            if user in question and any(keyword in question for keyword in ["æ˜¯èª°", "æ´»èº", "è¨Žè«–", "åƒèˆ‡", "è²¢ç»", "è¨Šæ¯"]):
                return True
        
        return False
    
    def _handle_user_activity_query(self, question: str) -> Dict[str, Any]:
        """è™•ç†ç”¨æˆ¶æ´»èºåº¦æŸ¥è©¢ - æ”¹é€²ç‰ˆæœ¬ï¼Œæä¾›è©³ç´°çš„ç”¨æˆ¶åˆ†æž"""
        try:
            import re
            import signal
            import time
            
            # æå–ç”¨æˆ¶åç¨± - æ”¹é€²çš„æ¨¡å¼åŒ¹é…
            user_name_patterns = [
                r'([^ï¼Ÿ?]+)(?:æ˜¯èª°|æ´»èº|è¨Žè«–|åƒèˆ‡|ç™¼éŽä»€éº¼|çš„è¨Šæ¯|çš„å…§å®¹)',
                r'(?:èª°æ˜¯|ä»‹ç´¹|åˆ†æž|æŸ¥è©¢)\s*([^ï¼Ÿ?]+)',
                r'([^ï¼Ÿ?]+)\s*(?:çš„æ´»èºåº¦|çš„è¨Žè«–|çš„åƒèˆ‡|çš„è¨Šæ¯|çš„å…§å®¹)',
                r'ç”¨æˆ¶\s*([^ï¼Ÿ?]+)',
                r'([^ï¼Ÿ?]+)\s*ç™¼éŽä»€éº¼',
                r'([^ï¼Ÿ?]+)\s*çš„è¨Šæ¯å…§å®¹'
            ]
            
            user_name = None
            for pattern in user_name_patterns:
                match = re.search(pattern, question)
                if match:
                    user_name = match.group(1).strip()
                    # æ¸…ç†ç”¨æˆ¶åç¨±
                    user_name = re.sub(r'[çš„èª°æ˜¯ï¼Ÿ?]', '', user_name).strip()
                    if user_name:
                        break
            
            if not user_name:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•å¾žæ‚¨çš„å•é¡Œä¸­è­˜åˆ¥å‡ºè¦æŸ¥è©¢çš„ç”¨æˆ¶åç¨±ã€‚è«‹æä¾›æ›´å…·é«”çš„ç”¨æˆ¶åç¨±ï¼Œä¾‹å¦‚ï¼š'Jesseæ˜¯èª°ï¼Ÿ' æˆ– 'Jesseç™¼éŽä»€éº¼è¨Šæ¯ï¼Ÿ'",
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": 0,
                    "context_length": 0
                }
            
            # è¨­ç½®æŸ¥è©¢è¶…æ™‚ï¼ˆ15ç§’ï¼‰
            def timeout_handler(signum, frame):
                raise TimeoutError("ç”¨æˆ¶æŸ¥è©¢è¶…æ™‚")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(15)  # 15ç§’è¶…æ™‚
            
            try:
                start_time = time.time()
                # ä½¿ç”¨æ”¹é€²çš„ç”¨æˆ¶åˆ†æž
                user_analysis = self._get_detailed_user_analysis(user_name)
                query_time = time.time() - start_time
                
                signal.alarm(0)  # å–æ¶ˆè¶…æ™‚
                
            except TimeoutError:
                signal.alarm(0)
                return {
                    "question": question,
                    "answer": f"æŸ¥è©¢ç”¨æˆ¶ {user_name} çš„è³‡è¨Šè¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«ç®¡ç†å“¡ã€‚",
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": 0,
                    "context_length": 0,
                    "timeout": True
                }
            
            if not user_analysis.get("user_found", False):
                return {
                    "question": question,
                    "answer": f"æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç”¨æˆ¶ '{user_name}' çš„ç›¸é—œä¿¡æ¯ã€‚è«‹ç¢ºèªç”¨æˆ¶åç¨±æ˜¯å¦æ­£ç¢ºã€‚",
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": 0,
                    "context_length": 0,
                    "query_time": query_time
                }
            
            # ç”Ÿæˆè©³ç´°çš„ç”¨æˆ¶åˆ†æžå ±å‘Š
            answer = self._generate_detailed_user_report(user_analysis)
            
            return {
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat(),
                "sources_used": 1,
                "context_length": len(answer),
                "user_analysis": user_analysis
            }
            
        except Exception as e:
            logger.error(f"è™•ç†ç”¨æˆ¶æ´»èºåº¦æŸ¥è©¢å¤±æ•—: {e}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œè™•ç†ç”¨æˆ¶æ´»èºåº¦æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
                "timestamp": datetime.now().isoformat(),
                "sources_used": 0,
                "context_length": 0
            }
    
    def _get_detailed_user_analysis(self, user_name: str) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„ç”¨æˆ¶åˆ†æžä¿¡æ¯"""
        try:
            from src.storage.connection_pool import get_db_connection, return_db_connection
            from psycopg2.extras import RealDictCursor
            import time
            
            start_time = time.time()
            
            conn = get_db_connection()
            cur = conn.cursor(cursor_factory=RealDictCursor)
            
            # æŸ¥æ‰¾ç”¨æˆ¶æ˜ å°„ - é¸æ“‡æœ‰æœ€å¤šæ•¸æ“šçš„ç”¨æˆ¶
            cur.execute("""
                WITH user_candidates AS (
                    SELECT 
                        unm.anonymized_id, 
                        unm.display_name, 
                        unm.real_name, 
                        unm.aliases, 
                        unm.group_terms,
                        COUNT(cd.id) as message_count
                    FROM user_name_mappings unm
                    LEFT JOIN community_data cd ON cd.author_anon = unm.anonymized_id AND cd.platform = 'slack'
                    WHERE unm.display_name ILIKE %s OR unm.real_name ILIKE %s 
                       OR %s = ANY(unm.aliases) OR %s = ANY(unm.group_terms)
                    GROUP BY unm.anonymized_id, unm.display_name, unm.real_name, unm.aliases, unm.group_terms
                    ORDER BY message_count DESC
                    LIMIT 1
                )
                SELECT anonymized_id, display_name, real_name, aliases, group_terms
                FROM user_candidates
            """, (f"%{user_name}%", f"%{user_name}%", user_name, user_name))
            
            user_result = cur.fetchone()
            if not user_result:
                cur.close()
                return_db_connection(conn)
                return {
                    "user_found": False,
                    "message": f"æœªæ‰¾åˆ°ç”¨æˆ¶ {user_name}",
                    "query_time": time.time() - start_time
                }
            
            anonymized_id = user_result['anonymized_id']
            display_name = user_result['display_name']
            real_name = user_result['real_name']
            
            # ç²å–ç”¨æˆ¶çµ±è¨ˆä¿¡æ¯
            cur.execute("""
                SELECT 
                    COUNT(*) as message_count,
                    COUNT(CASE WHEN metadata->>'thread_ts' IS NOT NULL THEN 1 END) as reply_count,
                    COUNT(CASE WHEN metadata->>'thread_ts' IS NULL OR metadata->>'thread_ts' = '' THEN 1 END) as main_message_count,
                    COUNT(DISTINCT metadata->>'channel') as channel_count,
                    MIN(timestamp) as first_activity,
                    MAX(timestamp) as last_activity,
                    COUNT(CASE WHEN metadata->>'emoji' IS NOT NULL THEN 1 END) as emoji_count
                FROM community_data 
                WHERE author_anon = %s AND platform = 'slack'
            """, (anonymized_id,))
            
            stats = cur.fetchone()
            
            # ç²å–é »é“æ´»èºåº¦
            cur.execute("""
                SELECT 
                    metadata->>'channel' as channel_id,
                    metadata->>'channel_name' as channel_name,
                    COUNT(*) as message_count
                FROM community_data 
                WHERE author_anon = %s AND platform = 'slack'
                GROUP BY metadata->>'channel', metadata->>'channel_name'
                ORDER BY message_count DESC
                LIMIT 5
            """, (anonymized_id,))
            
            channel_stats = cur.fetchall()
            
            # ç²å–æœ€è¿‘çš„è¨Šæ¯å…§å®¹
            cur.execute("""
                SELECT 
                    content,
                    timestamp,
                    metadata->>'channel_name' as channel_name
                FROM community_data 
                WHERE author_anon = %s AND platform = 'slack'
                ORDER BY timestamp DESC
                LIMIT 10
            """, (anonymized_id,))
            
            recent_messages = cur.fetchall()
            
            cur.close()
            return_db_connection(conn)
            
            query_time = time.time() - start_time
            
            return {
                "user_found": True,
                "display_name": display_name,
                "real_name": real_name,
                "anonymized_id": anonymized_id,
                "message_count": stats['message_count'],
                "reply_count": stats['reply_count'],
                "main_message_count": stats['main_message_count'],
                "channel_count": stats['channel_count'],
                "emoji_count": stats['emoji_count'],
                "first_activity": stats['first_activity'].isoformat() if stats['first_activity'] else None,
                "last_activity": stats['last_activity'].isoformat() if stats['last_activity'] else None,
                "channel_stats": [
                    {
                        "channel_id": ch['channel_id'],
                        "channel_name": ch['channel_name'],
                        "message_count": ch['message_count']
                    } for ch in channel_stats
                ],
                "recent_messages": [
                    {
                        "content": msg['content'],
                        "timestamp": msg['timestamp'].isoformat(),
                        "channel_name": msg['channel_name']
                    } for msg in recent_messages
                ],
                "query_time": query_time
            }
            
        except Exception as e:
            logger.error(f"ç²å–è©³ç´°ç”¨æˆ¶åˆ†æžå¤±æ•—: {e}")
            return {
                "user_found": False,
                "error": str(e),
                "query_time": time.time() - start_time if 'start_time' in locals() else 0
            }
    
    def _generate_detailed_user_report(self, user_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆè©³ç´°çš„ç”¨æˆ¶åˆ†æžå ±å‘Š"""
        try:
            display_name = user_analysis["display_name"]
            real_name = user_analysis.get("real_name", display_name)
            message_count = user_analysis["message_count"]
            reply_count = user_analysis["reply_count"]
            main_message_count = user_analysis["main_message_count"]
            channel_count = user_analysis["channel_count"]
            emoji_count = user_analysis["emoji_count"]
            last_activity = user_analysis["last_activity"]
            
            # æ§‹å»ºè©³ç´°å ±å‘Š
            report_parts = [
                f"## ðŸŽ¯ {display_name} æ˜¯èª°ï¼Ÿ",
                f"",
                f"**{display_name}** æ˜¯Apache Local Community Taipeiç¤¾ç¾¤ä¸­çš„ä¸€å€‹æ´»èºæˆå“¡ï¼",
                f""
            ]
            
            # åŸºæœ¬çµ±è¨ˆ
            report_parts.extend([
                f"### ðŸ“Š **åŸºæœ¬çµ±è¨ˆ**",
                f"- **ç¸½è¨Šæ¯æ•¸**ï¼š{message_count} æ¢",
                f"- **å›žè¦†æ•¸**ï¼š{reply_count} æ¬¡",
                f"- **ä¸»è¨Šæ¯æ•¸**ï¼š{main_message_count} æ¢",
                f"- **åƒèˆ‡é »é“æ•¸**ï¼š{channel_count} å€‹",
                f"- **Emojiä½¿ç”¨**ï¼š{emoji_count} æ¬¡",
                f"- **æœ€å¾Œæ´»å‹•æ™‚é–“**ï¼š{last_activity[:10] if last_activity else 'æœªçŸ¥'}",
                f""
            ])
            
            # é »é“æ´»èºåº¦
            if user_analysis.get("channel_stats"):
                report_parts.extend([
                    f"### ðŸ† **æœ€æ´»èºé »é“**",
                ])
                for i, channel in enumerate(user_analysis["channel_stats"], 1):
                    channel_name = channel.get("channel_name", f"é »é“{channel['channel_id']}")
                    message_count = channel["message_count"]
                    report_parts.append(f"{i}. **#{channel_name}**ï¼š{message_count} æ¢è¨Šæ¯")
                report_parts.append("")
            
            # æœ€è¿‘è¨Šæ¯å…§å®¹
            if user_analysis.get("recent_messages"):
                report_parts.extend([
                    f"### ðŸ’¬ **æœ€è¿‘è¨Šæ¯å…§å®¹**",
                ])
                for i, msg in enumerate(user_analysis["recent_messages"][:5], 1):
                    content = msg["content"][:100] + "..." if len(msg["content"]) > 100 else msg["content"]
                    timestamp = msg["timestamp"][:10] if msg["timestamp"] else "æœªçŸ¥æ™‚é–“"
                    channel_name = msg.get("channel_name", "æœªçŸ¥é »é“")
                    report_parts.append(f"{i}. **[{timestamp}]** åœ¨ #{channel_name}ï¼š")
                    report_parts.append(f"   {content}")
                    report_parts.append("")
            
            # å€‹æ€§ç‰¹å¾µåˆ†æž
            report_parts.extend([
                f"### ðŸ” **å€‹æ€§ç‰¹å¾µåˆ†æž**",
            ])
            
            # åŸºæ–¼æ•¸æ“šåˆ†æžå€‹æ€§ç‰¹å¾µ
            personality_traits = []
            
            if reply_count > main_message_count:
                personality_traits.append("æ´»èºçš„å›žè¦†è€…ï¼Œå–œæ­¡åƒèˆ‡è¨Žè«–")
            elif main_message_count > reply_count:
                personality_traits.append("ç¶“å¸¸ç™¼èµ·æ–°è©±é¡Œ")
            
            if channel_count > 3:
                personality_traits.append("åƒèˆ‡å¤šå€‹é »é“ï¼Œç¤¾ç¾¤åƒèˆ‡åº¦é«˜")
            
            if emoji_count > message_count * 0.3:
                personality_traits.append("å–œæ­¡ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œè¡¨é”æ–¹å¼ç”Ÿå‹•")
            
            if message_count > 50:
                personality_traits.append("ç¤¾ç¾¤ä¸­çš„æ´»èºæˆå“¡")
            elif message_count > 20:
                personality_traits.append("ä¸­ç­‰æ´»èºåº¦çš„åƒèˆ‡è€…")
            else:
                personality_traits.append("å¶çˆ¾åƒèˆ‡è¨Žè«–çš„æˆå“¡")
            
            for trait in personality_traits:
                report_parts.append(f"- {trait}")
            
            report_parts.extend([
                f"",
                f"### ðŸ“ˆ **æ´»èºåº¦æŽ’å**",
                f"æ ¹æ“šéŽåŽ»30å¤©çš„æ•¸æ“šï¼Œ{display_name} åœ¨ç¤¾ç¾¤ä¸­çš„æ´»èºåº¦æŽ’åéœ€è¦æŸ¥è©¢æ•´é«”çµ±è¨ˆä¾†ç¢ºå®šã€‚",
                f"",
                f"---",
                f"*ä»¥ä¸Šåˆ†æžåŸºæ–¼ {display_name} åœ¨Slackç¤¾ç¾¤ä¸­çš„å¯¦éš›æ´»å‹•æ•¸æ“š*"
            ])
            
            return "\n".join(report_parts)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè©³ç´°ç”¨æˆ¶å ±å‘Šå¤±æ•—: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç”¨æˆ¶åˆ†æžå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
    
    def _handle_stats_question(self, question: str) -> Dict[str, Any]:
        """è™•ç†çµ±è¨ˆé¡žå•é¡Œ"""
        try:
            # ç²å–Slackç”¨æˆ¶çµ±è¨ˆæ•¸æ“š
            user_stats = get_slack_user_stats(days_back=30, limit=3)
            activity_summary = get_slack_activity_summary(days_back=30)
            
            if not user_stats:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œç›®å‰æ²’æœ‰è¶³å¤ çš„Slackæ´»å‹•æ•¸æ“šä¾†çµ±è¨ˆæœ€æ´»èºçš„ä½¿ç”¨è€…ã€‚",
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": 0,
                    "context_length": 0
                }
            
            # ç”ŸæˆåŸºæ–¼å®¢è§€æ•¸æ“šçš„å›žç­”
            answer = self._generate_stats_answer(question, user_stats, activity_summary)
            
            return {
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat(),
                "sources_used": len(user_stats),
                "context_length": len(str(user_stats)),
                "stats_data": user_stats
            }
            
        except Exception as e:
            logger.error(f"è™•ç†çµ±è¨ˆå•é¡Œå¤±æ•—: {e}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œè™•ç†çµ±è¨ˆå•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "error": True,
                "timestamp": datetime.now().isoformat()
            }
    
    def _is_calendar_query(self, question: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ—¥æ›†æŸ¥è©¢"""
        calendar_keywords = [
            "æ—¥æ›†", "è¡Œäº‹æ›†", "æœƒè­°", "æ´»å‹•", "äº‹ä»¶", "æ™‚é–“", "ä»€éº¼æ™‚å€™", "ä½•æ™‚",
            "ç§‘æŠ€é–‹è¬›", "syncup", "sync up", "èšæœƒ", "è¬›åº§", "å·¥ä½œåŠ",
            "ä»Šå¤©", "æ˜Žå¤©", "æœ¬é€±", "ä¸‹é€±", "é€™å€‹æœˆ", "ä¸‹å€‹æœˆ"
        ]
        
        question_lower = question.lower()
        return any(keyword in question_lower for keyword in calendar_keywords)
    
    def _handle_calendar_query(self, question: str) -> Dict[str, Any]:
        """è™•ç†æ—¥æ›†æŸ¥è©¢"""
        try:
            import re
            from datetime import datetime, timedelta
            
            # æª¢æ¸¬æŸ¥è©¢é¡žåž‹
            if any(keyword in question.lower() for keyword in ["ç§‘æŠ€é–‹è¬›", "syncup", "sync up"]):
                # æœç´¢ç‰¹å®šäº‹ä»¶
                events = self.calendar_mcp.search_events("ç§‘æŠ€é–‹è¬›", limit=5)
                if not events:
                    events = self.calendar_mcp.search_events("syncup", limit=5)
            elif any(keyword in question.lower() for keyword in ["ä»Šå¤©", "ä»Šæ—¥"]):
                # ä»Šå¤©çš„äº‹ä»¶
                today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                tomorrow = today + timedelta(days=1)
                events = self.calendar_mcp.get_events_by_date_range(today, tomorrow, limit=10)
            elif any(keyword in question.lower() for keyword in ["æ˜Žå¤©", "æ˜Žæ—¥"]):
                # æ˜Žå¤©çš„äº‹ä»¶
                tomorrow = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
                day_after = tomorrow + timedelta(days=1)
                events = self.calendar_mcp.get_events_by_date_range(tomorrow, day_after, limit=10)
            elif any(keyword in question.lower() for keyword in ["æœ¬é€±", "é€™é€±"]):
                # æœ¬é€±çš„äº‹ä»¶
                today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                week_end = today + timedelta(days=7)
                events = self.calendar_mcp.get_events_by_date_range(today, week_end, limit=20)
            elif any(keyword in question.lower() for keyword in ["ä¸‹é€±", "ä¸‹é€±"]):
                # ä¸‹é€±çš„äº‹ä»¶
                next_week = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=7)
                week_after = next_week + timedelta(days=7)
                events = self.calendar_mcp.get_events_by_date_range(next_week, week_after, limit=20)
            elif any(keyword in question.lower() for keyword in ["11æœˆ", "åä¸€æœˆ"]):
                # 11æœˆçš„äº‹ä»¶
                start_date = datetime(2025, 11, 1).replace(tzinfo=timezone.utc)
                end_date = datetime(2025, 12, 1).replace(tzinfo=timezone.utc)
                events = self.calendar_mcp.get_events_by_date_range(start_date, end_date, limit=20)
            elif any(keyword in question.lower() for keyword in ["10æœˆ", "åæœˆ"]):
                # 10æœˆçš„äº‹ä»¶
                start_date = datetime(2025, 10, 1).replace(tzinfo=timezone.utc)
                end_date = datetime(2025, 11, 1).replace(tzinfo=timezone.utc)
                events = self.calendar_mcp.get_events_by_date_range(start_date, end_date, limit=20)
            elif any(keyword in question.lower() for keyword in ["12æœˆ", "åäºŒæœˆ"]):
                # 12æœˆçš„äº‹ä»¶
                start_date = datetime(2025, 12, 1).replace(tzinfo=timezone.utc)
                end_date = datetime(2026, 1, 1).replace(tzinfo=timezone.utc)
                events = self.calendar_mcp.get_events_by_date_range(start_date, end_date, limit=20)
            else:
                # é»˜èªç²å–æœªä¾†7å¤©çš„äº‹ä»¶
                events = self.calendar_mcp.get_upcoming_events(days_ahead=7, limit=10)
            
            if not events:
                return {
                    "question": question,
                    "answer": "ç›®å‰æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„æ—¥æ›†äº‹ä»¶ã€‚",
                    "timestamp": datetime.now().isoformat(),
                    "sources_used": 0,
                    "context_length": 0
                }
            
            # æ ¼å¼åŒ–äº‹ä»¶ä¿¡æ¯
            formatted_events = self.calendar_mcp.format_events_for_display(events)
            
            answer = f"ðŸ“… **æ—¥æ›†äº‹ä»¶æŸ¥è©¢çµæžœï¼š**\n\n{formatted_events}"
            
            return {
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat(),
                "sources_used": len(events),
                "context_length": len(answer),
                "calendar_events": [event.__dict__ for event in events]
            }
            
        except Exception as e:
            logger.error(f"è™•ç†æ—¥æ›†æŸ¥è©¢å¤±æ•—: {e}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œè™•ç†æ—¥æ›†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "sources_used": 0,
                "context_length": 0
            }
    
    def _generate_stats_answer(self, question: str, user_stats: List[Dict], activity_summary: Dict) -> str:
        """ç”ŸæˆåŸºæ–¼çµ±è¨ˆæ•¸æ“šçš„å›žç­”"""
        if not user_stats:
            return "ç›®å‰æ²’æœ‰è¶³å¤ çš„æ•¸æ“šä¾†å›žç­”é€™å€‹å•é¡Œã€‚"
        
        # ç°¡æ½”çš„å›žç­”æ ¼å¼
        answer_parts = []
        
        # æ·»åŠ æ‘˜è¦ä¿¡æ¯
        answer_parts.append(f"æ ¹æ“šéŽåŽ»30å¤©çš„å®¢è§€æ•¸æ“šçµ±è¨ˆï¼š")
        
        # åˆ—å‡ºæœ€æ´»èºçš„å‰ä¸‰åç”¨æˆ¶
        for i, user in enumerate(user_stats[:3], 1):
            # ç¢ºä¿ä½¿ç”¨çœŸå¯¦ç”¨æˆ¶åç¨±
            user_name = self.user_display_helper.get_display_name(user['user_name'], 'slack')
            message_count = user['message_count']
            reply_count = user['reply_count']
            emoji_given = user['emoji_given']
            
            answer_parts.append(
                f"{i}. **{user_name}** - ç™¼é€{message_count}æ¢è¨Šæ¯ï¼Œ"
                f"å›žè¦†{reply_count}æ¬¡ï¼Œçµ¦å‡º{emoji_given}å€‹emoji"
            )
        
        # æ·»åŠ ç¸½é«”çµ±è¨ˆ
        answer_parts.append(
            f"\nç¸½é«”çµ±è¨ˆï¼š{activity_summary['total_users']}ä½æ´»èºç”¨æˆ¶ï¼Œ"
            f"å…±{activity_summary['total_messages']}æ¢è¨Šæ¯ï¼Œ"
            f"å¹³å‡æ¯äºº{activity_summary['avg_messages_per_user']}æ¢è¨Šæ¯ã€‚"
        )
        
        return "\n".join(answer_parts)

    def ask_follow_up_question(
        self,
        question: str,
        k: int = 5,
        score_threshold: float = 0.7,
        source_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Ask a follow-up question with conversation context
        
        Args:
            question: The follow-up question
            k: Number of documents to retrieve
            score_threshold: Minimum similarity score for documents
            source_filter: Optional source type filter
            
        Returns:
            Dictionary with answer and context
        """
        try:
            # Get conversation history
            chat_history = self.memory.chat_memory.messages
            
            # Build context-aware query
            context_query = self._build_context_query(question, chat_history)
            
            # Ask question with context
            result = self.ask_question(
                question=context_query,
                k=k,
                score_threshold=score_threshold,
                source_filter=source_filter
            )
            
            # Update the result with the original question
            result["original_question"] = question
            result["context_query"] = context_query
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to answer follow-up question: {e}")
            return {
                "question": question,
                "answer": f"Sorry, I encountered an error while processing your follow-up question: {str(e)}",
                "error": True,
                "timestamp": datetime.now().isoformat()
            }

    def _build_context_query(self, question: str, chat_history: List[BaseMessage]) -> str:
        """
        Build a context-aware query from question and chat history
        
        Args:
            question: Current question
            chat_history: Previous conversation messages
            
        Returns:
            Context-aware query string
        """
        if not chat_history:
            return question
        
        # Extract recent questions and answers
        recent_context = []
        for i, message in enumerate(chat_history[-6:]):  # Last 3 Q&A pairs
            if isinstance(message, HumanMessage):
                recent_context.append(f"Previous question: {message.content}")
            elif isinstance(message, SystemMessage) and "answer" in message.content.lower():
                recent_context.append(f"Previous answer: {message.content}")
        
        if recent_context:
            context_str = "\n".join(recent_context)
            return f"Context from previous conversation:\n{context_str}\n\nCurrent question: {question}"
        
        return question

    def search_community_data(
        self,
        query: str,
        source_type: Optional[str] = None,
        limit: int = 10,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """
        Search community data without generating an answer
        
        Args:
            query: Search query
            source_type: Optional source type filter
            limit: Maximum number of results
            include_metadata: Whether to include full metadata
            
        Returns:
            Dictionary with search results
        """
        try:
            # Prepare filter
            filter_dict = None
            if source_type:
                filter_dict = {"source_type": source_type}
            
            # Perform search
            results = self.rag_system.similarity_search_with_score(
                query=query,
                k=limit,
                filter=filter_dict
            )
            
            # Format results
            search_results = []
            for doc, score in results:
                result_item = {
                    "content": doc.page_content,
                    "score": score,
                    "source_type": doc.metadata.get("source_type", "unknown")
                }
                
                if include_metadata:
                    result_item["metadata"] = doc.metadata
                
                search_results.append(result_item)
            
            return {
                "query": query,
                "results": search_results,
                "total_results": len(search_results),
                "source_filter": source_type,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Community data search failed: {e}")
            return {
                "query": query,
                "results": [],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """
        Get the current conversation history
        
        Returns:
            List of conversation messages
        """
        try:
            messages = self.memory.chat_memory.messages
            history = []
            
            for i, message in enumerate(messages):
                history.append({
                    "index": i,
                    "type": type(message).__name__,
                    "content": message.content,
                    "timestamp": datetime.now().isoformat()
                })
            
            return history
            
        except Exception as e:
            logger.error(f"Failed to get conversation history: {e}")
            return []

    def clear_conversation(self) -> bool:
        """
        Clear the conversation history
        
        Returns:
            True if successful
        """
        try:
            self.memory.clear()
            logger.info("Conversation history cleared")
            return True
            
        except Exception as e:
            logger.error(f"Failed to clear conversation: {e}")
            return False

    def get_system_stats(self) -> Dict[str, Any]:
        """
        Get system statistics
        
        Returns:
            Dictionary with system statistics
        """
        try:
            # Get RAG system stats
            rag_stats = self.rag_system.get_collection_stats()
            
            # Get conversation stats
            conversation_length = len(self.memory.chat_memory.messages)
            
            # Get LLM info
            llm_info = self.llm.get_model_info()
            
            return {
                "rag_system": rag_stats,
                "conversation_length": conversation_length,
                "llm_info": llm_info,
                "max_context_length": self.max_context_length,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get system stats: {e}")
            return {"error": str(e)}

    def test_system(self) -> Dict[str, Any]:
        """
        Test the Q&A system with a simple question
        
        Returns:
            Test results
        """
        try:
            test_question = "What is this community about?"
            
            # Test basic question answering
            result = self.ask_question(
                question=test_question,
                k=3,
                score_threshold=0.5
            )
            
            # Test system stats
            stats = self.get_system_stats()
            
            return {
                "test_question": test_question,
                "test_result": result,
                "system_stats": stats,
                "test_passed": not result.get("error", False),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"System test failed: {e}")
            return {
                "test_question": "What is this community about?",
                "test_result": {"error": str(e)},
                "test_passed": False,
                "timestamp": datetime.now().isoformat()
            }

            reply_count = user['reply_count']
            emoji_given = user['emoji_given']
            
            answer_parts.append(
                f"{i}. **{user_name}** - ç™¼é€{message_count}æ¢è¨Šæ¯ï¼Œ"
                f"å›žè¦†{reply_count}æ¬¡ï¼Œçµ¦å‡º{emoji_given}å€‹emoji"
            )
        
        # æ·»åŠ ç¸½é«”çµ±è¨ˆ
        answer_parts.append(
            f"\nç¸½é«”çµ±è¨ˆï¼š{activity_summary['total_users']}ä½æ´»èºç”¨æˆ¶ï¼Œ"
            f"å…±{activity_summary['total_messages']}æ¢è¨Šæ¯ï¼Œ"
            f"å¹³å‡æ¯äºº{activity_summary['avg_messages_per_user']}æ¢è¨Šæ¯ã€‚"
        )
        
        return "\n".join(answer_parts)

    def ask_follow_up_question(
        self,
        question: str,
        k: int = 5,
        score_threshold: float = 0.7,
        source_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Ask a follow-up question with conversation context
        
        Args:
            question: The follow-up question
            k: Number of documents to retrieve
            score_threshold: Minimum similarity score for documents
            source_filter: Optional source type filter
            
        Returns:
            Dictionary with answer and context
        """
        try:
            # Get conversation history
            chat_history = self.memory.chat_memory.messages
            
            # Build context-aware query
            context_query = self._build_context_query(question, chat_history)
            
            # Ask question with context
            result = self.ask_question(
                question=context_query,
                k=k,
                score_threshold=score_threshold,
                source_filter=source_filter
            )
            
            # Update the result with the original question
            result["original_question"] = question
            result["context_query"] = context_query
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to answer follow-up question: {e}")
            return {
                "question": question,
                "answer": f"Sorry, I encountered an error while processing your follow-up question: {str(e)}",
                "error": True,
                "timestamp": datetime.now().isoformat()
            }

    def _build_context_query(self, question: str, chat_history: List[BaseMessage]) -> str:
        """
        Build a context-aware query from question and chat history
        
        Args:
            question: Current question
            chat_history: Previous conversation messages
            
        Returns:
            Context-aware query string
        """
        if not chat_history:
            return question
        
        # Extract recent questions and answers
        recent_context = []
        for i, message in enumerate(chat_history[-6:]):  # Last 3 Q&A pairs
            if isinstance(message, HumanMessage):
                recent_context.append(f"Previous question: {message.content}")
            elif isinstance(message, SystemMessage) and "answer" in message.content.lower():
                recent_context.append(f"Previous answer: {message.content}")
        
        if recent_context:
            context_str = "\n".join(recent_context)
            return f"Context from previous conversation:\n{context_str}\n\nCurrent question: {question}"
        
        return question

    def search_community_data(
        self,
        query: str,
        source_type: Optional[str] = None,
        limit: int = 10,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """
        Search community data without generating an answer
        
        Args:
            query: Search query
            source_type: Optional source type filter
            limit: Maximum number of results
            include_metadata: Whether to include full metadata
            
        Returns:
            Dictionary with search results
        """
        try:
            # Prepare filter
            filter_dict = None
            if source_type:
                filter_dict = {"source_type": source_type}
            
            # Perform search
            results = self.rag_system.similarity_search_with_score(
                query=query,
                k=limit,
                filter=filter_dict
            )
            
            # Format results
            search_results = []
            for doc, score in results:
                result_item = {
                    "content": doc.page_content,
                    "score": score,
                    "source_type": doc.metadata.get("source_type", "unknown")
                }
                
                if include_metadata:
                    result_item["metadata"] = doc.metadata
                
                search_results.append(result_item)
            
            return {
                "query": query,
                "results": search_results,
                "total_results": len(search_results),
                "source_filter": source_type,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Community data search failed: {e}")
            return {
                "query": query,
                "results": [],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """
        Get the current conversation history
        
        Returns:
            List of conversation messages
        """
        try:
            messages = self.memory.chat_memory.messages
            history = []
            
            for i, message in enumerate(messages):
                history.append({
                    "index": i,
                    "type": type(message).__name__,
                    "content": message.content,
                    "timestamp": datetime.now().isoformat()
                })
            
            return history
            
        except Exception as e:
            logger.error(f"Failed to get conversation history: {e}")
            return []

    def clear_conversation(self) -> bool:
        """
        Clear the conversation history
        
        Returns:
            True if successful
        """
        try:
            self.memory.clear()
            logger.info("Conversation history cleared")
            return True
            
        except Exception as e:
            logger.error(f"Failed to clear conversation: {e}")
            return False

    def get_system_stats(self) -> Dict[str, Any]:
        """
        Get system statistics
        
        Returns:
            Dictionary with system statistics
        """
        try:
            # Get RAG system stats
            rag_stats = self.rag_system.get_collection_stats()
            
            # Get conversation stats
            conversation_length = len(self.memory.chat_memory.messages)
            
            # Get LLM info
            llm_info = self.llm.get_model_info()
            
            return {
                "rag_system": rag_stats,
                "conversation_length": conversation_length,
                "llm_info": llm_info,
                "max_context_length": self.max_context_length,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get system stats: {e}")
            return {"error": str(e)}

    def test_system(self) -> Dict[str, Any]:
        """
        Test the Q&A system with a simple question
        
        Returns:
            Test results
        """
        try:
            test_question = "What is this community about?"
            
            # Test basic question answering
            result = self.ask_question(
                question=test_question,
                k=3,
                score_threshold=0.5
            )
            
            # Test system stats
            stats = self.get_system_stats()
            
            return {
                "test_question": test_question,
                "test_result": result,
                "system_stats": stats,
                "test_passed": not result.get("error", False),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"System test failed: {e}")
            return {
                "test_question": "What is this community about?",
                "test_result": {"error": str(e)},
                "test_passed": False,
                "timestamp": datetime.now().isoformat()
            }
